# Tarea 02 â€“ MÃ©todos de Gran Escala

Esta carpeta contiene la **Tarea 02** del curso **MÃ©todos de Gran Escala**.  
El objetivo de esta tarea es implementar un **pipeline reproducible de datos y modelado**, siguiendo buenas prÃ¡cticas de ingenierÃ­a de datos y MLOps.

---

## Estructura del proyecto


tarea-02/
â”‚
â”œâ”€â”€ notebooks/          # Notebooks de exploraciÃ³n, EDA y prototipos
â”‚
â”œâ”€â”€ data/               # Datos del proyecto
â”‚   â”œâ”€â”€ raw/            # Datos originales (sin modificar)
â”‚   â”œâ”€â”€ prep/           # Datos preparados para modelado
â”‚   â”œâ”€â”€ inference/      # Datos para inferencia batch
â”‚   â””â”€â”€ predictions/    # Salidas de predicciÃ³n batch
â”‚
â”œâ”€â”€ src/                # CÃ³digo productivo (scripts)
â”‚   â”œâ”€â”€ etl.py          # Pipeline de extracciÃ³n y preparaciÃ³n
â”‚   â”œâ”€â”€ features.py     # Feature engineering
â”‚   â”œâ”€â”€ train.py        # Entrenamiento del modelo
â”‚   â””â”€â”€ predict.py      # Inferencia batch
â”‚
â”œâ”€â”€ artifacts/          # Artefactos generados
â”‚   â”œâ”€â”€ models/         # Modelos entrenados
â”‚   â”œâ”€â”€ reports/        # Reportes y grÃ¡ficos
â”‚   â””â”€â”€ metrics/        # MÃ©tricas y evaluaciones
â”‚
â”œâ”€â”€ pyproject.toml      # DefiniciÃ³n de dependencias
â”œâ”€â”€ uv.lock             # Lockfile para reproducibilidad
â””â”€â”€ README.md           # Este archivo

## Convenios del proyecto

Este proyecto sigue una serie de **convenios estÃ¡ndar** para asegurar orden, reproducibilidad y escalabilidad.

---

### ğŸ“‚ OrganizaciÃ³n de carpetas

- **`notebooks/`**
  - Uso exclusivo para:
    - EDA
    - AnÃ¡lisis exploratorio
    - Pruebas y prototipos
  - No debe contener lÃ³gica productiva final.

- **`src/`**
  - Contiene Ãºnicamente **cÃ³digo productivo**.
  - Cada script debe ser:
    - Reproducible
    - Ejecutable
    - Independiente del entorno interactivo.
  - No se ejecutan notebooks en producciÃ³n.

- **`data/`**
  - `raw/`: datos originales (solo lectura).
  - `prep/`: datos transformados y listos para modelado.
  - `inference/`: datos usados para predicciÃ³n batch.
  - `predictions/`: resultados de inferencia.

- **`artifacts/`**
  - Modelos entrenados
  - Reportes
  - GrÃ¡ficos
  - MÃ©tricas
  - Cualquier salida generada por el pipeline

---

### ğŸ§ª Manejo de datos

- Los datos **no se versionan** en Git.
- Solo se versiona la **estructura de carpetas**.
- El formato estÃ¡ndar para datos intermedios es **Parquet**.
- Los CSV solo se permiten en `raw/` si vienen de la fuente original.

---

### ğŸ“¦ Dependencias y entorno

- El proyecto utiliza **`uv`** para manejo de dependencias.
- Cada tarea/proyecto incluye:
  - `pyproject.toml`
  - `uv.lock`
- La instalaciÃ³n siempre se realiza con:

bash
uv sync

Este comando:
	â€¢	Crea el entorno virtual si no existe
	â€¢	Instala las dependencias definidas en pyproject.toml
	â€¢	Garantiza reproducibilidad usando uv.lock

## Detalle de la ejecuciÃ³n del proyecto

Esta secciÃ³n describe **cÃ³mo ejecutar el pipeline del proyecto** de forma correcta y reproducible, siguiendo los convenios definidos.

---

### ğŸ“ Punto de partida

Todos los comandos deben ejecutarse **desde la raÃ­z de la tarea**, por ejemplo:

bash
cd tareas/tarea-02
uv sync
uv run python src/etl.py
uv run python src/features.py
uv run python src/train.py
uv run python src/predict.py

Flujo recomendado: 

raw â†’ etl â†’ prep â†’ features â†’ train â†’ model â†’ predict â†’ predictions

## Autor

- Repositorio desarrollado como parte del curso MÃ©todos de Gran Escala.

JosÃ© Antonio Esparza
Gustavo Pardo